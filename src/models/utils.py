import os
import pickle as pkl
import time

import torch
import gpytorch
import matplotlib.pyplot as plt
import numpy as np
import math
import tqdm
import casadi as cs
from gpytorch.metrics import negative_log_predictive_density, mean_standardized_log_loss

from ds_utils import GP_DS
from common.kernel_utils import covSEard
from common.plotting_utils import plot_uncertainty_bounds_1d


def samplewise_pred_loss(mvnorm_inst, test_y, loss_type='nlpd'):
    """
    Adapted from gpytorch implementation to get samplewise prediction losses without averaging to be used for O.L. horizon.
    :param mvnorm_inst: multivariate normal distribution instance generated by likelihood(model(pred_x))
    :param test_y: Observed outputs whose probability of observance given the models we have trained is to be computed
    :param loss_type: Type of loss to be computed. Currently only nlpd is supported
    :return:
    """
    f_mean = mvnorm_inst.mean
    f_var = mvnorm_inst.variance
    # print(f_mean.shape, f_var.shape, test_y.shape)
    if loss_type == 'nlpd':
        return torch.diag(0.5 * torch.log(2 * math.pi * f_var) + torch.square(test_y - f_mean) / (2 * f_var))
    else:
        raise NotImplementedError


class GP_Model(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        """

        Parameters
        ----------
        train_x torch tensor of the shape (num_samples, input_dim)
        train_y torch tensor of the shape (num_samples, 1) since we assume all residual terms are independent and
                consists of stacks of 1-D outputs from independently trained GPs
        likelihood type of likelihood function to be used. Can vary depend on the kernel used
        """
        super(GP_Model, self).__init__(train_x, train_y, likelihood)
        self.train_x, self.train_y = train_x, train_y
        # Initialize the prior mean function to be a constant 0-vector'd mean as in Girard's paper. Constant mean priors can
        # also be learnt but they don't affect flexibility significantly for the posterior.
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())

    def forward(self, x):
        # Contains mean and covariance for the GP using the datapoints available.
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

    def _init_model(self):
        """Init GP model from train inputs and train_targets.

        """
        if self.train_y.ndim > 1:
            target_dimension = self.train_y.shape[1]
        else:
            target_dimension = 1

        # Extract dimensions for external use.
        self.input_dimension = self.train_x.shape[1]
        self.output_dimension = target_dimension
        self.n_training_samples = self.train_x.shape[0]

    def init_with_hyperparam(self):
        self._init_model()
        self._compute_GP_covariances()
        self.make_casadi_prediction_func()

    def _compute_GP_covariances(self):
        """
        Credit: safe-control-gym
        Compute K(X,X) + sigma*I and its inverse.
        """
        # Pre-compute inverse covariance plus noise to speed-up computation.
        K_lazy = self.covar_module(self.train_x)
        K_lazy_plus_noise = K_lazy.add_diag(self.likelihood.noise)
        n_samples = self.train_x.shape[0]
        # The fact that it's lazy means it isn't computed unless an operation is done involving the matrix. Here we simply
        # multiply by the identity matrix so it is no longer a lazy matrix and is actually computed for us.
        self.K_plus_noise = K_lazy_plus_noise.matmul(torch.eye(n_samples))  # Precomputes K_ztrain_ztrain
                                                                            # (denoted as K in equation 2.8 from Girard's paper)
        self.K_plus_noise_inv = K_lazy_plus_noise.inv_matmul(torch.eye(n_samples))

    def make_casadi_prediction_func(self):
        """
        Adapted from safe-control-gym
        """
        train_inputs, train_targets = self.train_x.numpy(), self.train_y.numpy()
        lengthscale = self.covar_module.base_kernel.lengthscale.detach().numpy()
        output_scale = self.covar_module.outputscale.detach().numpy()
        z = cs.SX.sym('z', self.input_dimension)
        # Compute \Sigma_{21} from https://peterroelants.github.io/posts/gaussian-process-tutorial/
        K_z_ztrain = cs.Function('k_z_ztrain', [z], [covSEard(z, train_inputs.T, lengthscale.T, output_scale)],
                                 ['z'], ['K'])
        # print(K_z_ztrain(z=z)['K'].shape, self.K_plus_noise_inv.detach().numpy().shape, train_targets.shape)
        # prediction function (posterior) for residual mean i.e. \mu_{2|1} from above link. Assumes prior mean is constant 0
        predict_mean = cs.Function('pred_cov', [z], [K_z_ztrain(z=z)['K'] @ self.K_plus_noise_inv.detach().numpy() @ train_targets.T],
                                   ['z'], ['mean'])
        # Compute \Sigma_{22}
        K_z_z = cs.Function('k_z_z', [z], [covSEard(z, z, lengthscale.T, output_scale)],
                            ['z'], ['K'])

        # prediction function (posterior) for residual covariance i.e. \Sigma{2|1}
        predict_cov = cs.Function('pred', [z], [cs.DM(self.likelihood.noise.item()) + K_z_z(z=z)['K'] - \
                                                (K_z_ztrain(z=z)['K'] @ self.K_plus_noise_inv.detach().numpy() @ (K_z_ztrain(z=z)['K']).T)],
                                  ['z'], ['cov'])
        # predict_cov = None
        self.cs_predict_mean, self.cs_predict_cov = predict_mean, predict_cov


def test_cs_preds(pw_gp_wrapped, gp_inp):
    models = pw_gp_wrapped.models
    true_means, *true_covs = pw_gp_wrapped(gp_inp)
    num_inps = gp_inp.shape[1]
    num_regions = len(models)
    print(true_covs)
    accumulated_covs = []
    for region_idx, model in enumerate(models):
        model.make_casadi_prediction_func()
        mean_manual = model.cs_predict_mean(gp_inp)
        cov_manual = model.cs_predict_cov(gp_inp)
        region_true_means = np.hstack([true_means[:, [k*num_regions + region_idx]] for k in range(num_inps)])
        # print(mean_manual.shape)
        # print(cov_manual)
        # print(cov_manual.shape)
        # print(true_covs[region_idx])
        accumulated_covs.append(cov_manual)

        assert np.isclose(mean_manual, region_true_means, atol=1e-4).all(), "Mean mismatch"
        assert np.isclose(cov_manual, true_covs[region_idx], atol=1e-4).all(), "Cov mismatch"

    print(accumulated_covs)
    Sigma_d_pw_arr = [cs.horzcat(*[accumulated_covs[r][:, k] for r in range(num_regions)]) for k in range(num_inps)]
    print(Sigma_d_pw_arr)


def train(model, likelihood_fn, optimizer, lossfn_callable, train_x, train_y, num_iter=400, verbose=True,
          return_trained_covs=False, terminate_by_change=False, ret_loss_only=False):
    loss_fn = lossfn_callable(likelihood_fn, model)

    if terminate_by_change:
        progress_iter = [1]*100000  # throwaway object since final number of iterations unknown
    else:
        progress_iter = range(num_iter)
    min_loss = 1e5
    prev_loss = 1e5
    loss_change_valid_iter = 0
    iterator = tqdm.tqdm(progress_iter) if verbose else progress_iter
    for i in iterator:
        optimizer.zero_grad()
        output = model(train_x)
        assert output.mean.dtype == train_y.dtype, "True labels and output labels must have matching dtypes in order to compute loss." \
                                                   "Current dtypes are: output.mean: %s, train_y: %s" % (
                                                   output.mean.dtype, train_y.dtype)
        loss = -loss_fn(output, train_y)
        loss.backward()
        curr_loss = loss.item()
        if terminate_by_change:
            if curr_loss < min_loss:
                min_loss = curr_loss
            perc_loss_change = np.abs((curr_loss - prev_loss)/prev_loss * 100)
            if perc_loss_change < 0.001:
                loss_change_valid_iter += 1
                if loss_change_valid_iter == 100:
                    break
            else:
                loss_change_valid_iter = 0
            prev_loss = curr_loss

        optimizer.step()

    # All kernels have a lengthscale parameter applied independently to each dimension as indicated here
    # https://docs.gpytorch.ai/en/v1.6.0/kernels.html
    if verbose:
        # print([model.covar_module.base_kernel.lengthscale], model.covar_module.base_kernel.lengthscale.shape)
        print('final loss: %s, lengthscale: %.5f noise variance (sigma**2): %.5f, noise std dev (sigma): %.5f' % (
            loss.item(), model.covar_module.base_kernel.lengthscale.item(), model.likelihood.noise.item(), math.sqrt(model.likelihood.noise.item())))

    if return_trained_covs:
        return math.sqrt(model.likelihood.noise.item())
    else:
        if ret_loss_only:
            return loss.item()
        else:
            return None


def get_gpds_attrs(gp_ds, no_squeeze=False):
    train_x = gp_ds.train_tensor.T
    if not no_squeeze:
        train_x = train_x.squeeze()
    return train_x, gp_ds.train_y.type(torch.FloatTensor), gp_ds.num_regions, gp_ds.output_dims


def train_test(gp_ds: GP_DS, no_squeeze=False, verbose=True, return_trained_covs=False, num_inducing_points=10,
               num_iter=400, terminate_by_change=False):
    '''
    This is the most common likelihood used for GP regression.
    (MLE of a Gaussian is Gaussian itself http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html).
    There are other options for exact GP regression, such as the [FixedNoiseGaussianLikelihood](http://docs.gpytorch.ai/likelihoods.html#fixednoisegaussianlikelihood),
    which assigns a different observed noise value to different training inputs but not sure if something like that would be of particular
    importance for a control system design when using a non-linear function to approximate residual dynamics.
    '''
    # print(gp_ds.train_x.shape)
    train_x, train_y, _, _ = get_gpds_attrs(gp_ds, no_squeeze=no_squeeze)
    # print("Test fn train_x shape: %s train_y shape: %s" % (train_x.shape, train_y.shape))
    likelihoods, independent_models = [], []
    for idx in range(train_y.shape[0]):
        i_dim_labels = train_y[idx, :].T
        # Squeeze is required other it yields an error saying that grad can be implicitly created only for scalar outputs.
        # Even though the ith row is scalar it still has a second unnecessary dimension so squeeze to remove.
        likelihood = gpytorch.likelihoods.GaussianLikelihood()
        model = GP_Model(train_x, i_dim_labels.squeeze(), likelihood)
        likelihoods.append(likelihood)
        independent_models.append(model)

    # Conventional pytorch training loop for hyperparameter optimization
    trained_covs = []
    for idx, model in enumerate(independent_models):
        likelihood = likelihoods[idx]
        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
        lossfn_callable = gpytorch.mlls.ExactMarginalLogLikelihood
        # Marginal loss as in Girard's paper
        trained_cov = train(model=model, likelihood_fn=likelihood,
                            optimizer=optimizer, lossfn_callable=lossfn_callable,
                            train_x=train_x, train_y=train_y[idx, :].T, verbose=verbose, return_trained_covs=return_trained_covs,
                            num_iter=num_iter, terminate_by_change=terminate_by_change)
        trained_covs.append(trained_cov)

    if not return_trained_covs:
        return likelihoods, independent_models
    else:
        return likelihoods, independent_models, trained_covs


def piecewise_train_test(gp_ds: GP_DS, no_squeeze=False, verbose=True, return_trained_covs=False,
                         num_iter=400, terminate_by_change=False, ret_loss_only=False):
    train_x, train_y, num_regions, dims = get_gpds_attrs(gp_ds, no_squeeze=no_squeeze)
    regionwise_sample_idxs = gp_ds.get_region_idxs()
    regions = gp_ds.regions

    likelihoods, models = [], []
    trained_covs = []
    summed_loss = 0
    for dim_idx in range(dims):
        for region_idx in range(len(regions)):
            if verbose:
                print("Training model: %s for region: %s" % (dim_idx + 1, region_idx + 1))
            likelihood = gpytorch.likelihoods.GaussianLikelihood()
            region_idx_samples = train_x[regionwise_sample_idxs[region_idx]]
            region_idx_labels = train_y[dim_idx, regionwise_sample_idxs[region_idx]]

            model = GP_Model(region_idx_samples, region_idx_labels, likelihood)
            # Hyperparam initialization
            model.covar_module.outputscale = 0.5
            model.likelihood.noise = 0.05
            likelihoods.append(likelihood)
            models.append(model)

            optimizer = torch.optim.Adam(model.parameters(), lr=0.05)
            lossfn_callable = gpytorch.mlls.ExactMarginalLogLikelihood


            train_op = train(model=model, likelihood_fn=likelihood,
                             optimizer=optimizer, lossfn_callable=lossfn_callable,
                             train_x=region_idx_samples, train_y=region_idx_labels, verbose=verbose,
                             return_trained_covs=return_trained_covs, num_iter=num_iter,
                             terminate_by_change=terminate_by_change, ret_loss_only=ret_loss_only)
            if return_trained_covs:
                trained_cov = train_op
                trained_covs.append(trained_cov)
            elif ret_loss_only:
                loss = train_op
                summed_loss += loss

    # print(likelihoods[0](models[0](train_x[regionwise_sample_idxs[0]])))
    if not return_trained_covs:
        if ret_loss_only:
            return summed_loss
        else:
            return likelihoods, models
    else:
        return likelihoods, models, trained_covs


def pw_gp_viz(pw_gp_wrapped, ds_ndim_test, ax, return_error_attrs=False, fineness_param=(100,)):
        # Generate fine grid to test closeness of predicted mean to true mean and also to plot predicted mean across the
        # state space.
        pred_ops = gen_pw_preds_and_mean_errors(pw_gp_wrapped.models, pw_gp_wrapped.likelihoods, pw_gp_wrapped,
                                                ds_ndim_test, return_error_attrs, fineness_param=fineness_param)
        if return_error_attrs:
            region_xs, observed_preds, fine_grid, observed_preds_full, mean_error_list, cov_list = pred_ops
        else:
            region_xs, observed_preds, fine_grid, observed_preds_full = pred_ops

        idx = -1
        colours = ['r', 'g', 'b']
        with torch.no_grad():
            for region_x, observed_pred in zip(region_xs, observed_preds):
                idx += 1
                if ds_ndim_test.input_dims == 2:
                    ax.scatter3D(region_x[0, :], region_x[1, :],
                                      observed_pred.mean.numpy(), color=colours[idx])
                if ds_ndim_test.input_dims == 1:
                    # axes[1].scatter(region_x[0, :], observed_pred.mean.numpy(), color=colours[idx])
                    plot_uncertainty_bounds_1d(observed_pred, region_x, ax, colours, idx)
        # with torch.no_grad():
        #     for region_x, observed_pred in zip(region_xs, observed_preds):
        #         idx += 1
        #         axes[1].scatter3D(region_x[0, :], region_x[1, :],
        #                           observed_pred.mean.numpy(), color=colours[idx])
        if return_error_attrs:
            return mean_error_list, cov_list


def gp_viz_plotter(ds_ndim_test: GP_DS, piecewise_fullgp_wrapped,
                   fineness_param=100, true_only=False, return_error_attrs=False):
        assert ds_ndim_test.output_dims == 1, "At the moment, can only visualize trained residuals for which output dims are 1."
        addn_rows = 0 if not return_error_attrs else 2
        fig = plt.figure(figsize=(15, 15))
        axes = []
        if ds_ndim_test.input_dims == 2:
            for i in range(3+addn_rows):
                ax = fig.add_subplot(3+addn_rows, 1, i+1, projection='3d')
                axes.append(ax)
            ds_ndim_test.viz_outputs_2d(fineness_param=fineness_param, ax=axes[0], true_only=true_only)
        elif ds_ndim_test.output_dims == 1:
            assert ds_ndim_test.region_gpinp_exclusive is True, "Can currently only visualize 1d outputs if region_gpinp_exclusive=True"
            for i in range(3+addn_rows):
                ax = fig.add_subplot(3+addn_rows, 1, i+1)
                axes.append(ax)
            ds_ndim_test.viz_outputs_1d_excl(fineness_param=fineness_param, ax=axes[0], true_only=true_only)
        else:
            raise ValueError("Can only visualize trained residuals for which input dims are 1 or 2.")

        ops = pw_gp_viz(pw_gp_wrapped=piecewise_fullgp_wrapped, ds_ndim_test=ds_ndim_test, ax=axes[1], return_error_attrs=False, fineness_param=(100,))
        if return_error_attrs:
            mean_error_list, cov_list = ops
            return fig, axes, mean_error_list, cov_list
        else:
            return fig, axes


class GPR_Callback(cs.Callback):
    def __init__(self, name, likelihood_fn, model, state_dim=1, output_dim=None, opts={}):
        """
        Parameters
        ----------
        name Name is necessary for Casadi initialization using construct.
        likelihood_fn
        model
        state_dim size of the input dimension.
        opts
        """
        cs.Callback.__init__(self)
        self.likelihood = likelihood_fn
        self.model = model
        self.input_dims = state_dim
        self.output_dims = output_dim
        if self.output_dims is None:
            self.output_dims = self.input_dims
        self.construct(name, opts)

    # Number of inputs and outputs
    def get_n_in(self): return 1
    def get_n_out(self): return 2

    def get_num_samples(self):
        return self.model.train_x.shape[0]

    def __len__(self): return 1

    def get_sparsity_in(self, i):
        # If this isn't specified then it doesn't accept the full input vector and only limits itself to the first element.
        # return cs.Sparsity.dense(self.state_dim, 1)
        return cs.Sparsity.dense(1, self.input_dims)

    def eval(self, arg):
        # likelihood will return a mean and variance but out differentiator only needs the mean
        # print(arg[0])
        mean, cov = self.postproc(self.likelihood(self.model(self.preproc(arg[0]))))
        return [mean, cov]

    @staticmethod
    def preproc(inp):
        return torch.from_numpy(np.array(inp).astype(np.float32))

    @staticmethod
    def postproc(op):
        # print(get_user_attributes(op))
        return op.mean.detach().numpy(), op.covariance_matrix.detach().numpy()


class Piecewise_GPR_Callback(GPR_Callback):
    def __init__(self, name, likelihood_fns, models, output_dim, input_dim, num_regions, opts={}):
        """

        Parameters
        ----------
        name
        likelihood_fns, models: For the multidim piecewise case the ordering would be all models belonging to one state dim
        first before moving to the next. Ex: for the 2-D case with 4 regions we have 8 models [model_0 ... model_7] where
        model_0-model_3 correspond to the 4 piecewise models for the first state dim and model_4-model_7 correspond to those for the
        second state dim
        output_dim: Dimension of the GP output. This must match with the number of models passed obeying the formula
        output_dim*num_regions = len(models)
        input_dim: Dimension of the GP input. Must match with the input dimension of each GP.
        num_regions: number of regions in the piecewise/hybrid model
        opts

        Returns
        Note that this function only returns a (horizontal) concatenation of the means output from each GP. The
        application of delta to select the right mean is left to a Casadi function defined in the piecewise MPC class.
        """
        cs.Callback.__init__(self)
        assert output_dim*num_regions == len(models), "The models must have length = output_dim*num_regions = %s but got %s instead" %\
                                                      (output_dim*num_regions, len(models))
        for model in models:
            assert input_dim == model.train_x.shape[-1], "The value of input_dim must match the number of columns of the model's train_x set. input_dim: %s, train_x_shape: %s" % (input_dim, model.train_x.shape)
        self.likelihoods = likelihood_fns
        self.models = models
        self.output_dims = output_dim
        self.input_dims = input_dim
        self.num_models = len(self.models)
        self.num_regions = num_regions
        self.construct(name, opts)
        self.organize_models()

    def get_n_in(self): return 1
    # Can't return covariances as list or 3-D tensor-like array. Instead return all cov matrices separately. There are
    # num_regions number of cov matrices to return.
    def get_n_out(self): return 1+self.num_regions

    def get_num_samples(self):
        # Only need to sum over 1 dimension of the output since the samples must be summed regionwise.
        return np.sum([self.models[idx].train_x.shape[0] for idx in range(len(self.models)//self.output_dims)])

    def get_sparsity_out(self, i):
        if i == 0:
            # Output mean shape is (self.output_dims, 1) and 1 for every region stacked horizontally to get the below
            return cs.Sparsity.dense(self.output_dims, self.num_regions)
        else:
            # output residual covariance matrices. One of these for every region for a total of self.num_regions covariance
            # matrices as evidenced by get_n_out
            return cs.Sparsity.dense(self.output_dims, self.output_dims)

    def __len__(self): return self.num_models

    def organize_models(self):
        # self.dimwise_region_models, self.dimwise_region_likelihoods = [[] for _ in range(self.num_regions)], [[] for _ in range(self.num_regions)]
        self.dimwise_region_models, self.dimwise_region_likelihoods = [[] for _ in range(self.output_dims)], [[] for _ in range(self.output_dims)]
        # Partition models per dimension. Ex: For 2-D case with 4 regions dimwise_models[0] has models[0]->models[3]
        # and dimwise_models[1] has models[4]->models[7]
        for output_dim in range(self.output_dims):
            self.dimwise_region_models[output_dim] = self.models[output_dim*(self.num_regions):
                                                                 (output_dim+1)*(self.num_regions)]
            self.dimwise_region_likelihoods[output_dim] = self.likelihoods[output_dim*(self.num_regions):
                                                                           (output_dim+1)*(self.num_regions)]

    def eval(self, arg):
        # Note regarding the covariances.
        # Regionwise_covs is going to be a list of single values corresponding to the covariance output in 1 region
        # of a single output GP. In the same way as the MultidimGPR callback, the covariance outputs from the same region
        # must be stored together in a diag matrix. Thus, the final covs list is going be a list of length = num_regions
        # with each element being a *diagonal* (because of independence across dims assumption in residual terms) matrix
        # of size (n_d, n_d)
        dimwise_means, dimwise_covs = [], []
        for output_dim in range(self.output_dims):
            regionwise_means, regionwise_covs = [], []
            dim_likelihoods, dim_models = self.dimwise_region_likelihoods[output_dim], self.dimwise_region_models[output_dim]
            for likelihood, model in zip(dim_likelihoods, dim_models):
                gp_op = self.postproc(likelihood(model(self.preproc(arg[0]))))
                # print(model(self.preproc(arg[0])).covariance_matrix)
                regionwise_means.append(gp_op[0])
                regionwise_covs.append(gp_op[1])
            # Note the gp output mean and covariances are of the shape (1,) (1,). When calling horzcat on these shapes, casadi ends up
            # up vertstacking them instead. So just use horzcat to generate a vector of shape (num_regions, 1) and then transpose. to
            # get the desired row vector instead of column vector.
            dimwise_means.append(cs.horzcat(regionwise_means).T)
            dimwise_covs.append(regionwise_covs)
        covs = [cs.diag([dim_cov[region_idx] for dim_cov in dimwise_covs]) for region_idx in range(self.num_regions)]
        means = cs.vertcat(*dimwise_means)
        return [means, *covs]

    def pred_likelihood(self, inp_samples, y_obsvd):
        # TODO: Currently not implemented for multi-dim residual output vectors.
        if len(self.dimwise_region_likelihoods) > 1:
            raise NotImplementedError
        samplewise_nll = np.zeros((inp_samples.shape[1], self.num_regions))
        for region_idx in range(self.num_regions):
            likelihood, model = self.dimwise_region_likelihoods[0][region_idx], self.dimwise_region_models[0][region_idx]
            gp_op_mvnorm_inst = likelihood(model(self.preproc(inp_samples.T)))
            with torch.no_grad():
                samplewise_loss = samplewise_pred_loss(gp_op_mvnorm_inst, y_obsvd.T)
                samplewise_nll[:, region_idx] = samplewise_loss
        # Normalize nll across samples since cross entropy loss requires row values to sum to 1.
        soft_labels = torch.softmax(-torch.from_numpy(samplewise_nll), dim=1)
        return soft_labels


class MultidimGPR_Callback(GPR_Callback):
    def __init__(self, name, likelihood_fns, models, state_dim=1, output_dim=None, opts={}):
        cs.Callback.__init__(self)
        self.likelihoods = likelihood_fns
        self.models = models
        self.n_d = len(self.models)
        self.state_dim = state_dim
        self.input_dims = self.state_dim
        self.output_dim = output_dim
        self.output_dims = output_dim
        if self.output_dim is None:
            self.output_dim = self.state_dim
        self.construct(name, opts)

    def get_n_in(self): return 1
    def get_n_out(self): return 2

    def __len__(self): return self.n_d

    def get_num_samples(self):
        # Number of samples is constant across all models for the multidimensional case unlike the piecewise one.
        return self.models[0].train_x.shape[0]

    def get_sparsity_out(self, i):
        if i == 0:
            return cs.Sparsity.dense(self.output_dim, 1)
        elif i == 1:
            return cs.Sparsity.dense(self.output_dim, self.output_dim)

    def eval(self, arg):
        means, covs = [], []
        for likelihood, model in zip(self.likelihoods, self.models):
            gp_op = self.postproc(likelihood(model(self.preproc(arg[0]))))
            means.append(gp_op[0])
            covs.append(gp_op[1])
        # Simplifying assumption that the residuals output in each dimension are independent of others and hence off-diagonal elements are 0.
        return [cs.vertcat(means), cs.diag(covs)]


def gen_pw_preds_and_mean_errors(models, likelihoods, gp_wrapped, ds_ndim_test: GP_DS, return_error_attrs, fineness_param):
    if ds_ndim_test.region_gpinp_exclusive:
        fine_grid = ds_ndim_test.generate_fine_grid(fineness_param=fineness_param, with_mask=False)
    else:
        fine_grid, mask = ds_ndim_test.generate_fine_grid(fineness_param=fineness_param, with_mask=True)
    region_xs, observed_preds, observed_preds_full = [], [], []
    if return_error_attrs:
        mean_error_list = []
        cov_list = []

    for model in models:
        model.init_with_hyperparam()

    for idx in range(len(models)):
        likelihood, model = likelihoods[idx], models[idx]
        # TODO: This assumes that each region only has a 1-D residual output associated with it. In the case where n_d > 1 this should be changed with circular indexing
        if ds_ndim_test.region_gpinp_exclusive:
            region_mask = ds_ndim_test.gen_excl_fine_mask(fine_grid, idx)
            region_test_samples = fine_grid
        else:
            region_mask = mask[idx]
            region_test_samples = fine_grid[:, region_mask.nonzero()[1]]
        region_idxs = np.nonzero(region_mask)[0]
        if return_error_attrs:
            region_errors = np.zeros([*region_idxs.shape])
        # print(region_test_samples)
        # print(type(region_test_samples))
        observed_pred = likelihood(model(GPR_Callback.preproc(region_test_samples.T)))
        # Check to ensure the callback method is working as intended
        with torch.no_grad():
            # callback sparsity is 1 sample at a time so need to iterate through all 1 at a time
            for sample_idx in range(region_test_samples.shape[-1]):
                sample = region_test_samples[:, sample_idx]
                residual_mean, *residual_covs = gp_wrapped(sample)
                true_mean = ds_ndim_test.generate_outputs(input_arr=torch.from_numpy(np.array(sample, ndmin=2).T), no_noise=True)
                non_callback_mean = observed_pred.mean.numpy()[sample_idx]
                if return_error_attrs:
                    cov_list.append((np.sqrt(residual_covs[idx]), idx))
                if return_error_attrs:
                    region_errors[sample_idx] = np.abs(true_mean - non_callback_mean)
                assert np.abs(residual_mean[:, idx] - non_callback_mean) <= 1e-3, \
                    "GP output mean (%s) and non-callback residual mean (%s) don't match: " % (residual_mean[:, idx], non_callback_mean)
        if return_error_attrs:
            mean_error_list.append({"region_samples": region_test_samples, "region_errors": region_errors})
        with torch.no_grad():
            observed_pred_full = likelihood(model(GPR_Callback.preproc(fine_grid.squeeze().T)))
        # print(observed_pred_full.mean.numpy().shape)
        observed_preds.append(observed_pred)
        observed_preds_full.append(observed_pred_full)
        region_xs.append(region_test_samples)

    if return_error_attrs:
        return region_xs, observed_preds, fine_grid, observed_preds_full, mean_error_list, cov_list
    else:
        return region_xs, observed_preds, fine_grid, observed_preds_full


def trained_gp_viz_test_no_hm(models, likelihoods, gp_ds: GP_DS, fineness_param=(21, 51), model_type='global'):
    colours = ['r', 'g', 'b', 'cyan']
    dims, num_regions = gp_ds.input_dims, gp_ds.num_regions
    num_plots = dims * 2
    fig = plt.figure(figsize=(16, 16))
    axes = []
    for plot_idx in range(num_plots):
        ax = fig.add_subplot(2, dims, plot_idx + 1, projection='3d')
        axes.append(ax)

    fine_grid, fine_regionwise_idxs = gp_ds.plot_true_func_2d(axes=axes[:dims], fineness_param=fineness_param,
                                                              true_plot=True, samples_plot=False, colours=colours)

    for model, likelihood in zip(models, likelihoods):
        model.eval()
        likelihood.eval()

    colours = gp_ds.get_colours(1 if model_type == 'global' else gp_ds.num_regions)

    with torch.no_grad():
        for dim_idx in range(dims):
            if model_type == "local":
                dim_models, dim_likelihoods = [temp[dim_idx * num_regions: (dim_idx + 1) * num_regions] for temp in
                                               [models, likelihoods]]
                for region_idx in range(len(gp_ds.regions)):
                    region_samples = gp_ds.ret_regspec_samples([fine_grid], region_idx, fine_regionwise_idxs)[0]
                    # print(type(region_samples), region_samples.shape)
                    region_ops = dim_likelihoods[region_idx](dim_models[region_idx](torch.from_numpy(region_samples.T)))
                    axes[dims + dim_idx].scatter3D(region_samples[0, :], region_samples[1, :],
                                                   region_ops.mean, color=colours[region_idx])
            else:
                # print(dim_idx)
                # print(type(models[dim_idx](torch.from_numpy(fine_grid.T))), type(models[dim_idx]))
                ops = likelihoods[dim_idx](models[dim_idx](torch.from_numpy(fine_grid.T)))
                axes[dims + dim_idx].scatter3D(fine_grid[0, :], fine_grid[1, :],
                                               ops.mean, color=colours[0])


def trained_gp_viz_test(models, likelihoods, gp_ds: GP_DS, fineness_param=(21, 51), model_type='global'):
    colours = ['r', 'g', 'b', 'cyan']
    dims, num_regions = gp_ds.input_dims, gp_ds.num_regions
    # First row plots true function, 2nd row plots learnt function, last row plots covariance heatmaps
    num_plots = dims * 3
    fig = plt.figure(figsize=(16, 16))
    axes = []
    for plot_idx in range(dims * 2):
        ax = fig.add_subplot(3, dims, plot_idx + 1, projection='3d')
        axes.append(ax)
    # Last row plots are 1-D
    for plot_idx in range(dims * 2, dims * 3):
        ax = fig.add_subplot(3, dims, plot_idx + 1)
        axes.append(ax)

    fine_grid, fine_regionwise_idxs = gp_ds.plot_true_func_2d(axes=axes[:dims], fineness_param=fineness_param,
                                                              true_plot=True, samples_plot=False, colours=colours)
    _, x_idxs = np.unique(fine_grid[0, :], return_index=True)
    _, y_idxs = np.unique(fine_grid[1, :], return_index=True)
    fine_x1_size, fine_x2_size = len(list(x_idxs)), len(list(y_idxs))
    # print(fine_x_size, fine_y_size)

    cov_mats = torch.zeros(fine_grid.shape)
    for model, likelihood in zip(models, likelihoods):
        model.eval()
        likelihood.eval()

    colours = gp_ds.get_colours(1 if model_type == 'global' else gp_ds.num_regions)

    with torch.no_grad():
        for dim_idx in range(dims):
            if model_type == "local":
                dim_models, dim_likelihoods = [temp[dim_idx * num_regions: (dim_idx + 1) * num_regions] for temp in
                                               [models, likelihoods]]
                for region_idx in range(len(gp_ds.regions)):
                    region_samples = gp_ds.ret_regspec_samples([fine_grid], region_idx, fine_regionwise_idxs)[0]
                    # print(type(region_samples), region_samples.shape)
                    # TODO: Add samples dots to plot.
                    region_ops = dim_likelihoods[region_idx](dim_models[region_idx](torch.from_numpy(region_samples.T)))
                    axes[dims + dim_idx].scatter3D(region_samples[0, :], region_samples[1, :],
                                                   region_ops.mean, color=colours[region_idx])
            else:
                # print(dim_idx)
                print(type(models[dim_idx](torch.from_numpy(fine_grid.T))), type(models[dim_idx]))
                ops = likelihoods[dim_idx](models[dim_idx](torch.from_numpy(fine_grid.T)))
                # ops = models[dim_idx](likelihoods[dim_idx](torch.from_numpy(fine_grid.T)))
                axes[dims + dim_idx].scatter3D(fine_grid[0, :], fine_grid[1, :],
                                               ops.mean, color=colours[0])
                # print(get_user_attributes(ops))
                print(type(ops.variance), ops.variance.shape, cov_mats[dim_idx, :].shape)
                cov_mats[dim_idx, :] += ops.variance

    for dim_idx in range(dims):
        cov_mat = cov_mats[dim_idx, :].reshape(fine_x1_size, fine_x2_size)
        # fine_x, fine_y = np.sort(np.unique(fine_grid[0, :])), np.sort(np.unique(fine_grid[1, :]))
        # print(fine_x, fine_y)
        # print(fine_x.shape, fine_y.shape)
        # axes[2*dims+dim_idx].pcolormesh(fine_x, fine_y, cov_mat[:-1, :-1],
        #                                 vmin=torch.min(cov_mat), vmax=torch.max(cov_mat),
        #                                 shading='flat')
        axes[2 * dims + dim_idx].pcolor(cov_mat)


def construct_pw_from_global(global_gp_wrapped: MultidimGPR_Callback, num_regions=3):
    likelihoods = global_gp_wrapped.likelihoods
    models = global_gp_wrapped.models
    res_output_dim = global_gp_wrapped.output_dim
    res_input_dim = global_gp_wrapped.input_dims
    num_regions = num_regions

    # Replicate
    likelihoods = likelihoods * num_regions
    models = models * num_regions

    for model in models:
        model.init_with_hyperparam()

    fake_pw_global_gp_wrapped = Piecewise_GPR_Callback('f', likelihoods, models,
                                                       output_dim=res_output_dim, input_dim=res_input_dim, num_regions=num_regions,
                                                       opts={"enable_fd": True})

    return fake_pw_global_gp_wrapped
